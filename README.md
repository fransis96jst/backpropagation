# backpropagation
utdi321

Kita punya jaringan saraf tiruan untuk XOR dengan arsitektur:   
<img src="img/arsitektur_jaringan_s1.png" width="500" alt="jaringan1">

- 2 neuron input: Xâ‚, Xâ‚‚  
- 1 bias input (selalu = 1)  
- 3 neuron hidden layer: Zâ‚, Zâ‚‚, Zâ‚ƒ  
- 1 neuron output: Yâ‚  
- Fungsi aktivasi: **sigmoid** di semua neuron (yang paling umum untuk backpropagation klasik)

Diketahui bobot awal:

**Tabel a** â†’ bobot dari input ke hidden layer (termasuk bias)

| Dari \ Ke | Zâ‚    | Zâ‚‚    | Zâ‚ƒ    |
|-----------|-------|-------|-------|
| Xâ‚        | -0.3  | 0.3   | -0.3  |
| Xâ‚‚        | 0.3   | -0.2  | 0.3   |
| bias (1)  | -0.1  | 0.3   | -0.1  |

**Tabel b** â†’ bobot dari hidden layer ke output (termasuk bias)

| Dari \ Ke | Yâ‚    |
|-----------|-------|
| Zâ‚        | -0.1  |
| Zâ‚‚        | 0.5   |
| Zâ‚ƒ        | -0.3  |
| bias (1)  | 0.2   |

Laju pembelajaran Î± = 0.2  
Pola pertama yang akan kita latih: **Xâ‚=1, Xâ‚‚=1, target t = 0** (karena 1 XOR 1 = 0)

### Langkah 1: Forward Pass

Hitung input ke setiap neuron hidden:

Zâ‚(in) = Xâ‚Â·wâ‚â‚ + Xâ‚‚Â·wâ‚‚â‚ + 1Â·biasâ‚ = (1)(-0.3) + (1)(0.3) + (1)(-0.1) = **-0.3 + 0.3 - 0.1 = -0.1**

Zâ‚‚(in) = (1)(0.3) + (1)(-0.2) + (1)(0.3) = 0.3 - 0.2 + 0.3 = **0.4**

Zâ‚ƒ(in) = (1)(-0.3) + (1)(0.3) + (1)(-0.1) = -0.3 + 0.3 - 0.1 = **-0.1**

Sekarang aktivasi sigmoid:

Ïƒ(x) = 1 / (1 + eâ»Ë£)

Zâ‚ = Ïƒ(-0.1) â‰ˆ 0.4750  
Zâ‚‚ = Ïƒ(0.4)  â‰ˆ 0.5987  
Zâ‚ƒ = Ïƒ(-0.1) â‰ˆ 0.4750

Hitung input ke output Yâ‚:

Yâ‚(in) = Zâ‚Â·vâ‚ + Zâ‚‚Â·vâ‚‚ + Zâ‚ƒÂ·vâ‚ƒ + biasÂ·vâ‚€  
= (0.4750)(-0.1) + (0.5987)(0.5) + (0.4750)(-0.3) + (1)(0.2)  
= -0.0475 + 0.29935 - 0.1425 + 0.2  
= **0.30935**

Yâ‚ = Ïƒ(0.30935) â‰ˆ **0.5768**

Jadi output jaringan saat ini â‰ˆ **0.577** (padahal targetnya 0 â†’ error besar)

### Langkah 2: Backpropagation â€“ Hitung Error dan Delta

Error di output:  
Î´_Y = (t - Yâ‚) Â· Yâ‚ Â· (1 - Yâ‚)  
= (0 - 0.5768) Â· 0.5768 Â· (1 - 0.5768)  
= (-0.5768) Â· 0.5768 Â· 0.4232 â‰ˆ **-0.1407**

Sekarang delta untuk setiap neuron hidden (Zâ‚, Zâ‚‚, Zâ‚ƒ):

Î´_Zj = Î´_Y Â· v_j Â· Zj Â· (1 - Zj)

Î´_Zâ‚ = -0.1407 Â· (-0.1) Â· 0.4750 Â· (1-0.4750) = 0.01407 Â· 0.4750 Â· 0.5250 â‰ˆ **0.00351**  
Î´_Zâ‚‚ = -0.1407 Â· (0.5)  Â· 0.5987 Â· (1-0.5987) = -0.07035 Â· 0.5987 Â· 0.4013 â‰ˆ **-0.01690**  
Î´_Zâ‚ƒ = -0.1407 Â· (-0.3) Â· 0.4750 Â· 0.5250 â‰ˆ 0.04221 Â· 0.4750 Â· 0.5250 â‰ˆ **0.01051**

### Langkah 3: Update Bobot (Î± = 0.2)

#### Update bobot dari hidden ke output (Tabel b)

Î”v_j = Î± Â· Î´_Y Â· Zj  
Î”v_bias = Î± Â· Î´_Y Â· 1

vâ‚ baru = -0.1 + 0.2 Â· (-0.1407) Â· 0.4750 â‰ˆ -0.1 - 0.01337 â‰ˆ **-0.1134**  
vâ‚‚ baru = 0.5 + 0.2 Â· (-0.1407) Â· 0.5987 â‰ˆ 0.5 - 0.01685 â‰ˆ **0.4832**  
vâ‚ƒ baru = -0.3 + 0.2 Â· (-0.1407) Â· 0.4750 â‰ˆ -0.3 - 0.01337 â‰ˆ **-0.3134**  
bias baru = 0.2 + 0.2 Â· (-0.1407) Â· 1 â‰ˆ 0.2 - 0.02814 â‰ˆ **0.1719**

#### Update bobot dari input ke hidden (Tabel a)

Î”w_ij = Î± Â· Î´_Zj Â· Xi  
(ingat Xâ‚=1, Xâ‚‚=1, bias=1)

**Ke Zâ‚ (Î´_Zâ‚ â‰ˆ 0.00351):**

w(Xâ‚â†’Zâ‚) = -0.3 + 0.2Â·0.00351Â·1 â‰ˆ -0.3 + 0.000702 â‰ˆ **-0.2993**  
w(Xâ‚‚â†’Zâ‚) = 0.3 + 0.000702 â‰ˆ **0.3007**  
w(biasâ†’Zâ‚) = -0.1 + 0.000702 â‰ˆ **-0.0993**

**Ke Zâ‚‚ (Î´_Zâ‚‚ â‰ˆ -0.01690):**

w(Xâ‚â†’Zâ‚‚) = 0.3 + 0.2Â·(-0.01690)Â·1 â‰ˆ 0.3 - 0.00338 â‰ˆ **0.2966**  
w(Xâ‚‚â†’Zâ‚‚) = -0.2 - 0.00338 â‰ˆ **-0.2034**  
w(biasâ†’Zâ‚‚) = 0.3 - 0.00338 â‰ˆ **0.2966**

**Ke Zâ‚ƒ (Î´_Zâ‚ƒ â‰ˆ 0.01051):**

w(Xâ‚â†’Zâ‚ƒ) = -0.3 + 0.2Â·0.01051Â·1 â‰ˆ -0.3 + 0.002102 â‰ˆ **-0.2979**  
w(Xâ‚‚â†’Zâ‚ƒ) = 0.3 + 0.002102 â‰ˆ **0.3021**  
w(biasâ†’Zâ‚ƒ) = -0.1 + 0.002102 â‰ˆ **-0.0979**

### Hasil Bobot Baru Setelah 1 Iterasi (pola Xâ‚=1, Xâ‚‚=1, t=0)

**Tabel a baru (input â†’ hidden):**

|       | Zâ‚     | Zâ‚‚     | Zâ‚ƒ     |
|-------|--------|--------|--------|
| Xâ‚    | -0.2993| 0.2966 | -0.2979|
| Xâ‚‚    | 0.3007 | -0.2034| 0.3021 |
| bias  | -0.0993| 0.2966 | -0.0979|

**Tabel b baru (hidden â†’ output):**

|       | Yâ‚     |
|-------|--------|
| Zâ‚    | -0.1134|
| Zâ‚‚    | 0.4832 |
| Zâ‚ƒ    | -0.3134|
| bias  | 0.1719 |

Catatan: Nilai di atas dibulatkan 4 desimal untuk kepraktisan. Kalau kamu butuh lebih presisi (misal 6-8 desimal), bisa kita hitung ulang.

Ini baru **satu iterasi** untuk pola (1,1) â†’ 0.  
Untuk melatih XOR sampai konvergen, kita harus ulangi proses ini secara berurutan/acak untuk keempat pola XOR (00â†’0, 01â†’1, 10â†’1, 11â†’0) ratusan/kali (biasanya 1000â€“10000 epoch) sampai error kecil.

Tapi sesuai permintaan soal: kamu minta hanya iterasi untuk pola pertama (1,1) dengan target 0 â†’ sudah selesai!

Kalau ada latihan lanjutan (pola kedua dst), bilang aja ya, kita lanjut! ğŸš€
